# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply prepend
# them with $. For strings the variable must be within quotes (ie, "$STR_VAR"),
# for numbers and booleans they should be plain (ie, $INT_VAR, $BOOL_VAR)


# Global tags can be specified here in key="value" format.
[global_tags]
  #Below are entirely used for telemetry
  agentversion = "$AGENT_VERSION"
  cluster = "$customResourceId"
  calias = "$AZMON_CLUSTER_ALIAS"
  clabel = "$AZMON_CLUSTER_LABEL"
  Region = "$AKSREGION"
  computer = "$NODE_NAME"
  nodeip = "$NODE_IP"
  mode = "$MODE"
  winmode = "$WINMODE"
  macmode = "$MAC"
  controllertype = "$CONTROLLER_TYPE"
  defaultmetricaccountname = "$AZMON_DEFAULT_METRIC_ACCOUNT_NAME"
  namespace = "$POD_NAMESPACE"
  podname = "$POD_NAME"
  ostype = "$OS_TYPE"
  mip = "$MINIMAL_INGESTION_PROFILE"
  operatormodel = "$AZMON_OPERATOR_ENABLED"
  operatormodelcfgmapsetting = "$AZMON_OPERATOR_ENABLED_CFG_MAP_SETTING"
  operatormodelchartsetting = "$AZMON_OPERATOR_ENABLED_CHART_SETTING"

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "60s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = false

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
  ## output, and will flush this buffer on a successful write. Oldest metrics
  ## are dropped first when this buffer fills.
  ## This buffer only fills when writes fail to output plugin(s).
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "30s"

  ## Default flushing interval for all outputs. You shouldn't set this below
  ## interval. Maximum flush_interval will be flush_interval + flush_jitter
  flush_interval = "15s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "15s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Logging configuration:
  ## Run telegraf with debug log messages.
  debug = false
  ## Run telegraf in quiet mode (error log messages only).
  quiet = true
  ## Specify the log file name. The empty string means to log to stderr.
  logfile = ""

  ## Override default hostname, if empty use os.Hostname()
  #hostname = "placeholder_hostname"
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = true


###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

[[outputs.application_insights]]
  ## Instrumentation key of the Application Insights resource.
  instrumentation_key = "$TELEMETRY_APPLICATIONINSIGHTS_KEY"

  ## Timeout for closing (default: 5s).
  # timeout = "5s"

  ## Enable additional diagnostic logging.
  # enable_diagnostic_logging = false


###############################################################################
#                            PROCESSOR PLUGINS                                #
###############################################################################
[[processors.converter]]
  [processors.converter.fields]
    float = ["*"]

# Transforms tag and field values as well as measurement, tag and field names with regex pattern
# [[processors.regex]]
#   namepass = ["target_allocator"]

#   # Tag and field conversions defined in a separate sub-tables
#   [[processors.regex.tags]]
#     ## Tag to change, "*" will change every tag
#     key = "job_name"
#     ## Regular expression to match on a tag value

#     # Group 1: match a string starting with "podMonitor/" or "serviceMonitor/"
#     #          The format of a CR name is podMonitor/<namespace>/<podmonitor-name> or serviceMonitor/<namespace>/<servicemonitor-name>
#     # Group 2: match the string "podMonitor" or "serviceMonitor"
#     # Group 3: match any string. We want all job names to be matched so that we aren't collecting the job name.
#     pattern = '(^(podMonitor|serviceMonitor)\/.*)|(.*)'
#     ## Matches of the pattern will be replaced with this string.  Use ${1}
#     ## notation to use the text of the first submatch.
    
#     ## "Job" is necessary or else configmap jobs would not be replaced and would keep their
#     ## original name
#     replacement = "${2}Job"

###############################################################################
#                            AGGREGATOR PLUGINS                               #
###############################################################################
[[aggregators.quantile]]
  period = "5m"
  drop_original = true
  quantiles = [0.50,0.95]
  algorithm = "t-digest"
  compression = 100.0
  namepass = ["otelcollector", "metricsextension"]

###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

# Read metrics about cpu usage
#[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
#  percpu = false
  ## Whether to report total system cpu stats or not
#  totalcpu = true
  ## If true, collect raw CPU time metrics.
#  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states.
#  report_active = true
#  fieldpass = ["usage_active","cluster","node","host","device"]
#  taginclude = ["cluster","cpu","node"]
  
# Read metrics about memory usage
#[[inputs.mem]]
#  fieldpass = ["used_percent", "cluster", "node","host","device"]
#  taginclude = ["cluster","node"]

[[inputs.procstat]]
   exe = "otelcollector"
   interval = "10s"
   pid_finder = "pgrep"
   pid_tag = true
   name_override = "otelcollector"
   fieldpass = ["cpu_usage", "memory_rss"]
   [inputs.procstat.tags]
#    Computer = "$NODE_NAME"
#    NodeIp = "$NODE_IP"
    cpulimit = "$CONTAINER_CPU_LIMIT"
    memlimit = "$CONTAINER_MEMORY_LIMIT"
    
    defaultscrapekubelet = "$AZMON_PROMETHEUS_KUBELET_SCRAPING_ENABLED"
    defaultscrapecoreDns = "$AZMON_PROMETHEUS_COREDNS_SCRAPING_ENABLED"
    defaultscrapecadvisor = "$AZMON_PROMETHEUS_CADVISOR_SCRAPING_ENABLED"
    defaultscrapekubeproxy = "$AZMON_PROMETHEUS_KUBEPROXY_SCRAPING_ENABLED"
    defaultscrapeapiserver = "$AZMON_PROMETHEUS_APISERVER_SCRAPING_ENABLED"
    defaultscrapekubestate = "$AZMON_PROMETHEUS_KUBESTATE_SCRAPING_ENABLED"
    defaultscrapenodeexporter = "$AZMON_PROMETHEUS_NODEEXPORTER_SCRAPING_ENABLED"
    defaultscrapecollectorhealth = "$AZMON_PROMETHEUS_COLLECTOR_HEALTH_SCRAPING_ENABLED"
    defaultscrapewindowsexporter = "$AZMON_PROMETHEUS_WINDOWSEXPORTER_SCRAPING_ENABLED"
    defaultscrapewindowskubeproxy = "$AZMON_PROMETHEUS_WINDOWSKUBEPROXY_SCRAPING_ENABLED"
    defaultscrapepodannotations = "$AZMON_PROMETHEUS_POD_ANNOTATION_SCRAPING_ENABLED"
    podannotationns = "$AZMON_PROMETHEUS_POD_ANNOTATION_NAMESPACES_REGEX"
    defaultscrapekappiebasic = "$AZMON_PROMETHEUS_KAPPIEBASIC_SCRAPING_ENABLED"
    nodeexportertargetport= "$NODE_EXPORTER_TARGETPORT"
    nodeexportername = "$NODE_EXPORTER_NAME"
    kubestatename = "$KUBE_STATE_NAME"
    kubestateversion = "$KUBE_STATE_VERSION"
    operatortargetstaimgversion = "$OPERATOR_TARGETS_TA_IMG_VERSION"
    operatortargetscfgreaderimgversion = "$OPERATOR_TARGETS_CFG_READER_IMG_VERSION"
    nodeexporterversion = "$NODE_EXPORTER_VERSION"
    akvauth = "$AKVAUTH"
    debugmodeenabled = "$DEBUG_MODE_ENABLED"
    kubestatemetriclabelsallowlist = "$KUBE_STATE_METRIC_LABELS_ALLOWLIST"
    kubestatemetricannotationsallowlist = "$KUBE_STATE_METRIC_ANNOTATIONS_ALLOWLIST"
    httpproxyenabled = "$HTTP_PROXY_ENABLED"
    tadapterh="$tokenadapterHealthyAfterSecs"
    tadapterf="$tokenadapterUnhealthyAfterSecs"
  
[[inputs.procstat]]
   exe = "MetricsExtension"
   interval = "10s"
   pid_finder = "pgrep"
   pid_tag = true
   name_override = "metricsextension"
   fieldpass = ["cpu_usage", "memory_rss"]
   
[[inputs.prometheus]]
  interval = "5m"
  urls = ["http://localhost:8888/metrics"]
  fieldpass = ["otelcol_processor_dropped_metric_points", "otelcol_receiver_refused_metric_points", "otelcol_receiver_accepted_metric_points", "otelcol_exporter_sent_metric_points", "otelcol_exporter_queue_size", "otelcol_exporter_send_failed_metric_points", "otelcol_process_memory_rss", "otelcol_processor_batch_batch_send_size_bytes_sum", "otelcol_processor_batch_batch_send_size_bytes_count"]
  tagexclude = ["service_instance_id"]
  metric_version = 2
  url_tag = "scrapeUrl"
  timeout = "15s"

[[inputs.prometheus]]
  interval = "5m"
  urls = ["http://localhost:9090/metrics"]
  fieldpass = ["prometheus_sd_http_failures_total"]
  metric_version = 2
  url_tag = "scrapeUrl"
  timeout = "15s"

[[inputs.prometheus]]
  interval = "5m"
  urls = ["http://ama-metrics-operator-targets.kube-system.svc.cluster.local/metrics"]
  fieldpass = ["opentelemetry_allocator_targets","opentelemetry_allocator_collectors_discovered"]
  metric_version = 2
  url_tag = "scrapeUrl"
  timeout = "15s"
  name_override = "target_allocator"
